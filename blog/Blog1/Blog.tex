\documentclass[11pt]{article}\textwidth 6.5in\textheight 9in
\usepackage{amssymb}\usepackage[colorlinks]{hyperref}\usepackage{color}
	\usepackage{stmaryrd}\usepackage{mathrsfs}
		\usepackage{graphicx}
	\usepackage{amsmath}
\usepackage{caption}
    \usepackage{subcaption}
    \usepackage{listings}
 
\topmargin -3pc\oddsidemargin 0in\evensidemargin 0in\begin{document}
\setlength{\captionmargin}{27pt}
\newcommand\hreff[1]{\href {http://#1} {\small http://#1}}
\newcommand\trm[1]{{\bf\em #1}} \newcommand\emm[1]{{\ensuremath{#1}}}
\newcommand\prf{\paragraph{Proof.}}\newcommand\qed{\hfill\emm\blacksquare}

\newtheorem{thr}{Theorem} 
\newtheorem{lmm}{Lemma}
%\newtheorem{st}{Statement}
\newtheorem{cor}{Corollary}
\newtheorem{con}{Conjecture} 
\newtheorem{prp}{Proposition}

\newtheorem{blk}{Block}
\newtheorem{dff}{Definition}
\newtheorem{asm}{Assumption}
\newtheorem{rmk}{Remark}
\newtheorem{clm}{Claim}
\newtheorem{exm}{Example}

\newcommand\Ks{\mathbf{Ks}} 
\newcommand{\ab}{a\!b}
\newcommand{\yx}{y\!x}
\newcommand{\yux}{y\!\underline{x}}

\newcommand\floor[1]{{\lfloor#1\rfloor}}\newcommand\ceil[1]{{\lceil#1\rceil}}

%\newcommand\lea{\,{\leq^+}\,}\newcommand\gea{\,{\geq^+}\,}\newcommand\eqa{\,{=^+}\,}
%\newcommand\lel{{\,\leq^{\log}\,}}\newcommand\gel{{\,\geq^{\log}\,}}


\newcommand{\lea}{<^+}
\newcommand{\gea}{>^+}
\newcommand{\eqa}{=^+}

%\newcommand{\lea}{\stackrel{+}{<}}
%\newcommand{\gea}{\stackrel{+}{>}}
%\newcommand{\eqa}{\stackrel{+}{=}}

%\newcommand{\lel}{\stackrel{\log}{<}}
%\newcommand{\gel}{\stackrel{\log}{>}}
%\newcommand{\eql}{\stackrel{\log}{=}}

\newcommand{\lel}{<^{\log}}
\newcommand{\gel}{>^{\log}}
\newcommand{\eql}{=^{\log}}

\newcommand{\lem}{\stackrel{\ast}{<}}
\newcommand{\gem}{\stackrel{\ast}{>}}
\newcommand{\eqm}{\stackrel{\ast}{=}}

\newcommand\edf{{\,\stackrel{\mbox{\tiny def}}=\,}}
\newcommand\edl{{\,\stackrel{\mbox{\tiny def}}\leq\,}}
\newcommand\then{\Rightarrow}


\newcommand\C{\mathbf{C}} 

\renewcommand\chi{\mathcal{H}}
\newcommand\km{{\mathbf {km}}}\renewcommand\t{{\mathbf {t}}}
\newcommand\KM{{\mathbf {KM}}}\newcommand\m{{\mathbf {m}}}
\newcommand\md{{\mathbf {m}_{\mathbf{d}}}}\newcommand\mT{{\mathbf {m}_{\mathbf{T}}}}
\newcommand\K{{\mathbf K}} \newcommand\I{{\mathbf I}}

\newcommand\II{\hat{\mathbf I}}
\newcommand\Kd{{\mathbf{Kd}}} \newcommand\KT{{\mathbf{KT}}} 
\renewcommand\d{{\mathbf d}} 
\newcommand\D{{\mathbf D}}

\newcommand\w{{\mathbf w}}
\newcommand\Cs{\mathbf{Cs}} \newcommand\q{{\mathbf q}}
\newcommand\E{{\mathbf E}} \newcommand\St{{\mathbf S}}
\newcommand\M{{\mathbf M}}\newcommand\Q{{\mathbf Q}}
\newcommand\ch{{\mathcal H}} \renewcommand\l{\tau}
\newcommand\tb{{\mathbf t}} \renewcommand\L{{\mathbf L}}
\newcommand\bb{{\mathbf {bb}}}\newcommand\Km{{\mathbf {Km}}}
\renewcommand\q{{\mathbf q}}\newcommand\J{{\mathbf J}}
\newcommand\z{\mathbf{z}}

\newcommand\B{\mathbf{bb}}\newcommand\f{\mathbf{f}}
\newcommand\hd{\mathbf{0'}} \newcommand\T{{\mathbf T}}
\newcommand\R{\mathbb{R}}\renewcommand\Q{\mathbb{Q}}
\newcommand\N{\mathbb{N}}\newcommand\BT{\{0,1\}}
\newcommand\FS{\BT^*}\newcommand\IS{\BT^\infty}
\newcommand\FIS{\BT^{*\infty}}
\renewcommand\S{\mathcal{C}}\newcommand\ST{\mathcal{S}}
\newcommand\UM{\nu_0}\newcommand\EN{\mathcal{W}}

\newcommand{\supp}{\mathrm{Supp}}

\newcommand\lenum{\lbrack\!\lbrack}
\newcommand\renum{\rbrack\!\rbrack}

\newcommand\h{\mathbf{h}}
\renewcommand\qed{\hfill\emm\square}
\renewcommand\i{\mathbf{i}}
\newcommand\p{\mathbf{p}}
\renewcommand\q{\mathbf{q}}
\title{ AIT Blog}

\author {Samuel Epstein\\samepst@jptheorygroup.org}

\maketitle

This is a math blog focusing on Algorithmic Information Theory. The main focus will be on strings $x\in\FS$ that have low mutual information with the halting sequence $H$, with $\I(x;H)=\K(x)-\K(x|H)$, being low. $\K$ is the prefix free Kolmogorov complexity. There are many properties that can be proven about elementary objects that have low $\I(x;H)$. We say an object is (non)exotic if it has (low)high mutual information with the halting sequence. Exotic objects cannot be found in the physical world. Furthermore, $\I(x;H)$ enjoys conservation laws, in that deterministic and random processing cannot increase information.
\begin{itemize}
\item For partial computable $f$, $\I(f(a):H)< \I(a;H)+\K(f)+O(1)$.
\item For program $q$ that computes probability $p$ over $\N$, $\E_{a\sim p}[2^{\I(\langle q,a\rangle;H)}]< O(1)2^{\I(q;H)}$.
\end{itemize}
This entry deals with the relationship between Algorithithmic Information Theory and Machine Learning. Classification is the task of learning a binary function $c$  from $\N$ to bits $\BT$. The learner is given a sample consisting of pairs $(x,b)$ for string $x$ and bit $b$ and outputs a binary classifier $h:\N\rightarrow\BT$ that should match $c$ as much as possible. Occam's razor says that ``the simplest explanation is usually the best one.'' Simple hypothesis are resilient against overfitting to the sample data. With certain probabilistic assumptions, learning algorithms that produce hypotheses of low Kolmogorov complexity are likely to correctly predict the target function~\cite{BlumerEhHaWar89}. The following theorem shows that the samples can be compressed to their count.

\begin{thr}
Given a set of samples $\{(x_i,b_i)\}$, $i=1,\dots,n$, there is a total function $f:\N\rightarrow\BT$ such that $f(x_i)=b_i$ for $i=1,\dots,n$ and $\K(f)<^{\log} n + \I(\{(x_i,b_i)\};H)$.
\end{thr}


However, usually the samples can be modeled as coming from a probabilistic model. The target concept is modeled by a random variable $X$ with distribution $p$ over ordered lists of natural numbers. The random variable $Y$ models the labels, and has a distribution over lists of bits, where the distribution of $X\times Y$ is $p(x,y)$ with conditional probability requirement $p(y|x)=\prod_{i=1..|x|}p(y_i|x_i)$. Each such $(x_i,y_i)$ is a labeled sample. A binary classifier $f$ is consistent with labelled samples $(x,y)$, if for all $i$, $f(x_i)=y_i$. Let $\Gamma(x,y)$ be the minimum Kolmogorov complexity of a classifier consistent with $(x,y)$. $Entropy(Y|X)$ is the conditional entropy of $Y$ given $X$.

\begin{thr} $Entropy(Y|X)\leq\E[\Gamma(X,Y)]\lel Entropy(Y|X)+\K(p)$.
\end{thr}

Another area of machine learning is regression, in which one is give a set of pairs $\{(x_i,y_i)\}$, $i=1\dots n$, and the goal is to find a function $f$, such that $f(x_i)=y_i$. Usually each $x_i$ and $y_i$ represents a point in Euclidean space, but for our purpose they are natural numbers. As in classification, the goal is to use Occam's razor to find the simpliest function, to prevent overfitting to the random noise inherent in the sample data. The following theorem provides bounds on the simplest function completely consistent with the data.
\begin{thr}
For $\{(x_i,y_i)\}$, $i=1,\dots,n$,  there exists $f:\N\rightarrow\N$ with $f(x_i)= y_i$ for $i\in\{1,\dots,n\}$ and $\K(f) \lel \sum_{i=1}^n\K(y_i|x_i)+\I(\{(x_i,y_i)\};H)$.
\end{thr}

This theorem can be proved using Theorem 8 in \cite{EpsteinDerandom22}. However, this theorem is over computable probability measures, whereas the lower semi-computable $\m$ is needed. By using so-called left-total machines, $\m$ can be converted into a computable measure. In fact one of the benefits of using left-total machines and having $\I(,;H)$ as an error term, is that semi-computable functions can be converted into computable ones.
\bibliographystyle{alpha} 
\bibliography{refnotes}

\end{document}
