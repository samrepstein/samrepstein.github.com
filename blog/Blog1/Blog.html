<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title> AIT Blog</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="Blog.tex"> 
<link rel="stylesheet" type="text/css" href="Blog.css"> 
</head><body 
>
   <div class="maketitle">
                                                                                         
                                                                                         
                                                                                         
                                                                                         

<h2 class="titleHead"> AIT Blog</h2>
      <div class="author" ><span 
class="cmr-12">Samuel Epstein</span>
<br /><span 
class="cmr-12">samepst@jptheorygroup.org</span></div><br />
<div class="date" ><span 
class="cmr-12">September 28, 2022</span></div>
   </div>
<!--l. 114--><p class="indent" >   This is a math blog focusing on Algorithmic Information Theory. The main focus will be
on strings <span 
class="cmmi-10x-x-109">x </span><span 
class="cmsy-10x-x-109">&#x2208;{</span>0<span 
class="cmmi-10x-x-109">,</span>1<span 
class="cmsy-10x-x-109">}</span><sup><span 
class="cmsy-8">*</span></sup> that have low mutual information with the halting sequence <span 
class="cmmi-10x-x-109">H</span>, with
<span 
class="cmbx-10x-x-109">I</span>(<span 
class="cmmi-10x-x-109">x</span>;<span 
class="cmmi-10x-x-109">H</span>) = <span 
class="cmbx-10x-x-109">K</span>(<span 
class="cmmi-10x-x-109">x</span>) <span 
class="cmsy-10x-x-109">- </span><span 
class="cmbx-10x-x-109">K</span>(<span 
class="cmmi-10x-x-109">x</span><span 
class="cmsy-10x-x-109">|</span><span 
class="cmmi-10x-x-109">H</span>), being low. <span 
class="cmbx-10x-x-109">K </span>is the prefix free Kolmogorov complexity. There are many
properties that can be proven about elementary objects that have low <span 
class="cmbx-10x-x-109">I</span>(<span 
class="cmmi-10x-x-109">x</span>;<span 
class="cmmi-10x-x-109">H</span>). We say an object is
(non)exotic if it has (low)high mutual information with the halting sequence. Exotic objects cannot be
found in the physical world. Furthermore, <span 
class="cmbx-10x-x-109">I</span>(<span 
class="cmmi-10x-x-109">x</span>;<span 
class="cmmi-10x-x-109">H</span>) enjoys conservation laws, in that deterministic and
random processing cannot increase information.
     <ul class="itemize1">
     <li class="itemize">For partial computable <span 
class="cmmi-10x-x-109">f</span>, <span 
class="cmbx-10x-x-109">I</span>(<span 
class="cmmi-10x-x-109">f</span>(<span 
class="cmmi-10x-x-109">a</span>) : <span 
class="cmmi-10x-x-109">H</span>) <span 
class="cmmi-10x-x-109">&#x003C; </span><span 
class="cmbx-10x-x-109">I</span>(<span 
class="cmmi-10x-x-109">a</span>;<span 
class="cmmi-10x-x-109">H</span>) + <span 
class="cmbx-10x-x-109">K</span>(<span 
class="cmmi-10x-x-109">f</span>) + <span 
class="cmmi-10x-x-109">O</span>(1).
     </li>
     <li class="itemize">For program <span 
class="cmmi-10x-x-109">q </span>that computes probability <span 
class="cmmi-10x-x-109">p </span>over <span 
class="msbm-10x-x-109">&#x2115;</span>, <span 
class="cmbx-10x-x-109">E</span><sub><span 
class="cmmi-8">a</span><span 
class="cmsy-8">~</span><span 
class="cmmi-8">p</span></sub>[2<sup><span 
class="cmbx-8">I</span><span 
class="cmr-8">(</span><span 
class="cmsy-8">&#x27E8;</span><span 
class="cmmi-8">q,a</span><span 
class="cmsy-8">&#x27E9;</span><span 
class="cmr-8">;</span><span 
class="cmmi-8">H</span><span 
class="cmr-8">)</span></sup>] <span 
class="cmmi-10x-x-109">&#x003C; O</span>(1)2<sup><span 
class="cmbx-8">I</span><span 
class="cmr-8">(</span><span 
class="cmmi-8">q</span><span 
class="cmr-8">;</span><span 
class="cmmi-8">H</span><span 
class="cmr-8">)</span></sup>.</li></ul>
<!--l. 119--><p class="noindent" >This entry deals with the relationship between Algorithithmic Information Theory and Machine
Learning. Classification is the task of learning a binary function <span 
class="cmmi-10x-x-109">c </span>from <span 
class="msbm-10x-x-109">&#x2115; </span>to bits <span 
class="cmsy-10x-x-109">{</span>0<span 
class="cmmi-10x-x-109">,</span>1<span 
class="cmsy-10x-x-109">}</span>. The
learner is given a sample consisting of pairs (<span 
class="cmmi-10x-x-109">x,b</span>) for string <span 
class="cmmi-10x-x-109">x </span>and bit <span 
class="cmmi-10x-x-109">b </span>and outputs a binary
classifier <span 
class="cmmi-10x-x-109">h </span>: <span 
class="msbm-10x-x-109">&#x2115; </span><span 
class="cmsy-10x-x-109">&#x2192;{</span>0<span 
class="cmmi-10x-x-109">,</span>1<span 
class="cmsy-10x-x-109">} </span>that should match <span 
class="cmmi-10x-x-109">c </span>as much as possible. Occam&#8217;s razor says that
&#8220;the simplest explanation is usually the best one.&#8221; Simple hypothesis are resilient against
overfitting to the sample data. With certain probabilistic assumptions, learning algorithms that
produce hypotheses of low Kolmogorov complexity are likely to correctly predict the target
function&#x00A0;<span class="cite">[<a 
href="#XBlumerEhHaWar89">BEHW89</a>]</span>. The following theorem shows that the samples can be compressed to their
count.
   <div class="newtheorem">
<!--l. 121--><p class="noindent" ><span class="head">
<span 
class="cmbx-10x-x-109">Theorem</span><span 
class="cmbx-10x-x-109">&#x00A0;1</span> </span><span 
class="cmti-10x-x-109">Given a set of samples </span><span 
class="cmsy-10x-x-109">{</span>(<span 
class="cmmi-10x-x-109">x</span><sub><span 
class="cmmi-8">i</span></sub><span 
class="cmmi-10x-x-109">,b</span><sub><span 
class="cmmi-8">i</span></sub>)<span 
class="cmsy-10x-x-109">}</span><span 
class="cmti-10x-x-109">, </span><span 
class="cmmi-10x-x-109">i </span>= 1<span 
class="cmmi-10x-x-109">,</span><span 
class="cmmi-10x-x-109">&#x2026;</span><span 
class="cmmi-10x-x-109">,n</span><span 
class="cmti-10x-x-109">, there is a total function </span><span 
class="cmmi-10x-x-109">f </span>: <span 
class="msbm-10x-x-109">&#x2115; </span><span 
class="cmsy-10x-x-109">&#x2192;{</span>0<span 
class="cmmi-10x-x-109">,</span>1<span 
class="cmsy-10x-x-109">}</span>
<span 
class="cmti-10x-x-109">such that </span><span 
class="cmmi-10x-x-109">f</span>(<span 
class="cmmi-10x-x-109">x</span><sub><span 
class="cmmi-8">i</span></sub>) = <span 
class="cmmi-10x-x-109">b</span><sub><span 
class="cmmi-8">i</span></sub> <span 
class="cmti-10x-x-109">for </span><span 
class="cmmi-10x-x-109">i </span>= 1<span 
class="cmmi-10x-x-109">,</span><span 
class="cmmi-10x-x-109">&#x2026;</span><span 
class="cmmi-10x-x-109">,n </span><span 
class="cmti-10x-x-109">and </span><span 
class="cmbx-10x-x-109">K</span>(<span 
class="cmmi-10x-x-109">f</span>) <span 
class="cmmi-10x-x-109">&#x003C;</span> log <span 
class="cmmi-10x-x-109">n </span>+ <span 
class="cmbx-10x-x-109">I</span>(<span 
class="cmsy-10x-x-109">{</span>(<span 
class="cmmi-10x-x-109">x</span><sub><span 
class="cmmi-8">i</span></sub><span 
class="cmmi-10x-x-109">,b</span><sub><span 
class="cmmi-8">i</span></sub>)<span 
class="cmsy-10x-x-109">}</span>;<span 
class="cmmi-10x-x-109">H</span>)<span 
class="cmti-10x-x-109">.</span>
   </div>
<!--l. 126--><p class="indent" >   However, usually the samples can be modeled as coming from a probabilistic model. The
target concept is modeled by a random variable <span 
class="cmmi-10x-x-109">X </span>with distribution <span 
class="cmmi-10x-x-109">p </span>over ordered lists of
natural numbers. The random variable <span 
class="cmmi-10x-x-109">Y </span>models the labels, and has a distribution over lists
                                                                                         
                                                                                         
of bits, where the distribution of <span 
class="cmmi-10x-x-109">X </span><span 
class="cmsy-10x-x-109">&#x00D7; </span><span 
class="cmmi-10x-x-109">Y </span>is <span 
class="cmmi-10x-x-109">p</span>(<span 
class="cmmi-10x-x-109">x,y</span>) with conditional probability requirement
<span 
class="cmmi-10x-x-109">p</span>(<span 
class="cmmi-10x-x-109">y</span><span 
class="cmsy-10x-x-109">|</span><span 
class="cmmi-10x-x-109">x</span>) = <span 
class="cmex-10x-x-109">&#x220F;</span>
  <sub><span 
class="cmmi-8">i</span><span 
class="cmr-8">=1</span><span 
class="cmmi-8">..</span><span 
class="cmsy-8">|</span><span 
class="cmmi-8">x</span><span 
class="cmsy-8">|</span></sub><span 
class="cmmi-10x-x-109">p</span>(<span 
class="cmmi-10x-x-109">y</span><sub><span 
class="cmmi-8">i</span></sub><span 
class="cmsy-10x-x-109">|</span><span 
class="cmmi-10x-x-109">x</span><sub><span 
class="cmmi-8">i</span></sub>). Each such (<span 
class="cmmi-10x-x-109">x</span><sub><span 
class="cmmi-8">i</span></sub><span 
class="cmmi-10x-x-109">,y</span><sub><span 
class="cmmi-8">i</span></sub>) is a labeled sample. A binary classifier <span 
class="cmmi-10x-x-109">f </span>is consistent
with labelled samples (<span 
class="cmmi-10x-x-109">x,y</span>), if for all <span 
class="cmmi-10x-x-109">i</span>, <span 
class="cmmi-10x-x-109">f</span>(<span 
class="cmmi-10x-x-109">x</span><sub><span 
class="cmmi-8">i</span></sub>) = <span 
class="cmmi-10x-x-109">y</span><sub><span 
class="cmmi-8">i</span></sub>. Let &#x0393;(<span 
class="cmmi-10x-x-109">x,y</span>) be the minimum Kolmogorov
complexity of a classifier consistent with (<span 
class="cmmi-10x-x-109">x,y</span>). <span 
class="cmmi-10x-x-109">Entropy</span>(<span 
class="cmmi-10x-x-109">Y </span><span 
class="cmsy-10x-x-109">|</span><span 
class="cmmi-10x-x-109">X</span>) is the conditional entropy of <span 
class="cmmi-10x-x-109">Y </span>given
<span 
class="cmmi-10x-x-109">X</span>.
   <div class="newtheorem">
<!--l. 128--><p class="noindent" ><span class="head">
<span 
class="cmbx-10x-x-109">Theorem</span><span 
class="cmbx-10x-x-109">&#x00A0;2</span> </span><span 
class="cmmi-10x-x-109">Entropy</span>(<span 
class="cmmi-10x-x-109">Y </span><span 
class="cmsy-10x-x-109">|</span><span 
class="cmmi-10x-x-109">X</span>) <span 
class="cmsy-10x-x-109">&#x2264; </span><span 
class="cmbx-10x-x-109">E</span>[&#x0393;(<span 
class="cmmi-10x-x-109">X,Y </span>)] <span 
class="cmmi-10x-x-109">&#x003C;</span> <sup><span 
class="cmr-8">log</span></sup><span 
class="cmmi-10x-x-109">Entropy</span>(<span 
class="cmmi-10x-x-109">Y </span><span 
class="cmsy-10x-x-109">|</span><span 
class="cmmi-10x-x-109">X</span>) + <span 
class="cmbx-10x-x-109">K</span>(<span 
class="cmmi-10x-x-109">p</span>)<span 
class="cmti-10x-x-109">.</span>
   </div>
<!--l. 131--><p class="indent" >   Another area of machine learning is regression, in which one is give a set of pairs <span 
class="cmsy-10x-x-109">{</span>(<span 
class="cmmi-10x-x-109">x</span><sub><span 
class="cmmi-8">i</span></sub><span 
class="cmmi-10x-x-109">,y</span><sub><span 
class="cmmi-8">i</span></sub>)<span 
class="cmsy-10x-x-109">}</span>, <span 
class="cmmi-10x-x-109">i </span>= 1<span 
class="cmmi-10x-x-109">&#x2026;</span><span 
class="cmmi-10x-x-109">n</span>,
and the goal is to find a function <span 
class="cmmi-10x-x-109">f</span>, such that <span 
class="cmmi-10x-x-109">f</span>(<span 
class="cmmi-10x-x-109">x</span><sub><span 
class="cmmi-8">i</span></sub>) = <span 
class="cmmi-10x-x-109">y</span><sub><span 
class="cmmi-8">i</span></sub>. Usually each <span 
class="cmmi-10x-x-109">x</span><sub><span 
class="cmmi-8">i</span></sub> and <span 
class="cmmi-10x-x-109">y</span><sub><span 
class="cmmi-8">i</span></sub> represents a point in
Euclidean space, but for our purpose they are natural numbers. As in classification, the goal is to use
Occam&#8217;s razor to find the simpliest function, to prevent overfitting to the random noise inherent in the
sample data. The following theorem provides bounds on the simplest function completely consistent with
the data. <div class="newtheorem">
<!--l. 132--><p class="noindent" ><span class="head">
<span 
class="cmbx-10x-x-109">Theorem</span><span 
class="cmbx-10x-x-109">&#x00A0;3</span> </span><span 
class="cmti-10x-x-109">For </span><span 
class="cmsy-10x-x-109">{</span>(<span 
class="cmmi-10x-x-109">x</span><sub><span 
class="cmmi-8">i</span></sub><span 
class="cmmi-10x-x-109">,y</span><sub><span 
class="cmmi-8">i</span></sub>)<span 
class="cmsy-10x-x-109">}</span><span 
class="cmti-10x-x-109">, </span><span 
class="cmmi-10x-x-109">i </span>= 1<span 
class="cmmi-10x-x-109">,</span><span 
class="cmmi-10x-x-109">&#x2026;</span><span 
class="cmmi-10x-x-109">,n</span><span 
class="cmti-10x-x-109">, there exists </span><span 
class="cmmi-10x-x-109">f </span>: <span 
class="msbm-10x-x-109">&#x2115; </span><span 
class="cmsy-10x-x-109">&#x2192; </span><span 
class="msbm-10x-x-109">&#x2115; </span><span 
class="cmti-10x-x-109">with </span><span 
class="cmmi-10x-x-109">f</span>(<span 
class="cmmi-10x-x-109">x</span><sub><span 
class="cmmi-8">i</span></sub>) = <span 
class="cmmi-10x-x-109">y</span><sub><span 
class="cmmi-8">i</span></sub> <span 
class="cmti-10x-x-109">for </span><span 
class="cmmi-10x-x-109">i </span><span 
class="cmsy-10x-x-109">&#x2208;{</span>1<span 
class="cmmi-10x-x-109">,</span><span 
class="cmmi-10x-x-109">&#x2026;</span><span 
class="cmmi-10x-x-109">,n</span><span 
class="cmsy-10x-x-109">} </span><span 
class="cmti-10x-x-109">and</span>
<span 
class="cmbx-10x-x-109">K</span>(<span 
class="cmmi-10x-x-109">f</span>) <span 
class="cmmi-10x-x-109">&#x003C;</span> <sup><span 
class="cmr-8">log</span></sup> <span 
class="cmex-10x-x-109">&#x2211;</span>
    <sub><span 
class="cmmi-8">i</span><span 
class="cmr-8">=1</span></sub><sup><span 
class="cmmi-8">n</span></sup><span 
class="cmbx-10x-x-109">K</span>(<span 
class="cmmi-10x-x-109">y</span><sub><span 
class="cmmi-8">i</span></sub><span 
class="cmsy-10x-x-109">|</span><span 
class="cmmi-10x-x-109">x</span><sub><span 
class="cmmi-8">i</span></sub>) + <span 
class="cmbx-10x-x-109">I</span>(<span 
class="cmsy-10x-x-109">{</span>(<span 
class="cmmi-10x-x-109">x</span><sub><span 
class="cmmi-8">i</span></sub><span 
class="cmmi-10x-x-109">,y</span><sub><span 
class="cmmi-8">i</span></sub>)<span 
class="cmsy-10x-x-109">}</span>;<span 
class="cmmi-10x-x-109">H</span>)<span 
class="cmti-10x-x-109">.</span>
   </div>
<!--l. 136--><p class="indent" >   This theorem can be proved using Theorem 8 in <span class="cite">[<a 
href="#XEpsteinDerandom22">Eps22</a>]</span>. However, this theorem is over computable
probability measures, whereas the lower semi-computable <span 
class="cmbx-10x-x-109">m </span>is needed. By using so-called left-total
machines, <span 
class="cmbx-10x-x-109">m </span>can be converted into a computable measure. In fact one of the benefits of using left-total
machines and having <span 
class="cmbx-10x-x-109">I</span>(<span 
class="cmmi-10x-x-109">,</span>;<span 
class="cmmi-10x-x-109">H</span>) as an error term, is that semi-computable functions can be converted into
computable ones.
   <h3 class="likesectionHead"><a 
 id="x1-1000"></a>References</h3>
<!--l. 1--><p class="noindent" >
           <div class="thebibliography">
           <p class="bibitem" ><span class="biblabel">
 [BEHW89]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XBlumerEhHaWar89"></a>A.&#x00A0;Blumer, A.&#x00A0;Ehrenfeucht, D.&#x00A0;Haussler, and M.&#x00A0;K. Warmuth.  Learnability and
           the Vapnik-Chervonenkis dimension. <span 
class="cmti-10x-x-109">Journal of the ACM</span>, 36(4):929&#8211;965, 1989.
           </p>
           <p class="bibitem" ><span class="biblabel">
 [Eps22]   <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XEpsteinDerandom22"></a>S.&#x00A0;Epstein. The outlier theorem revisited. <span 
class="cmti-10x-x-109">CoRR</span>, abs/2203.08733, 2022.
</p>
           </div>
    
</body></html> 

                                                                                         


